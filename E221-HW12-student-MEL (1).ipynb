{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f621cd5a",
   "metadata": {},
   "source": [
    "# ENGR-E 221 Intelligent Systems I Fall 2023\n",
    "\n",
    "## Homeworkr 12 Reinforcement learning - Frozen Lake - 50 points\n",
    "\n",
    "**Due December 6, 2023 at 11:59 pm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb72bb71",
   "metadata": {},
   "source": [
    "OpenAI Gym is a free Python toolkit that provides developers with an environment for developing and testing learning agents for deep learning models. It’s useful as a reinforcement learning agent, but it’s also adept at testing new learning agent ideas, running training simulations and speeding up the learning process for your algorithm.  \n",
    "\n",
    "During class, you were introduced to the Frozen Lake environment. Then, during lab, you saw different ways to implement Reinforcement Learning algorithms to optimize agent success in different environments (Cart Pole and Black Jack). This homework gives you the opportunity to explore Frozen Lake and understand several RL techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cbd265",
   "metadata": {},
   "source": [
    "## Part 1. Load Frozen Lake and Explore (5 pts)\n",
    "You may either use the script version provided in class or the Jupyter Notebook code here. Recall that if you are trying to render the environment, it will need to be done differently depending on whether you use Jupyter Notebook or another IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb636d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\",render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset()\n",
    "#environment.render()\n",
    "\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3da6768",
   "metadata": {},
   "source": [
    "## Environment\n",
    "What observations can be made about the environment? What does env.P give you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code and/or discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd85056d",
   "metadata": {},
   "source": [
    "## Actions\n",
    "What actions are available in this environment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6d0b1",
   "metadata": {},
   "source": [
    "## Perform an Action\n",
    "Perform a single action. Render the the elf's movement. What variables are returned? What do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23edabac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code and discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee4b169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "771590d0",
   "metadata": {},
   "source": [
    "## Part 2. Random Action (10 pt)\n",
    "\n",
    "Using randomly chosen actions, iterate through 1000 steps. How well did your elf perform? Use the code block below to help you assess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b63ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(env, pol_func, episodes):\n",
    "  misses = 0\n",
    "  steps_list = []\n",
    "  all_steps_list = []\n",
    "  for episode in range(episodes):\n",
    "    observation = env.reset()\n",
    "    steps=0\n",
    "    while True:\n",
    "      \n",
    "      action = pol_func(observation)\n",
    "      returnValue = env.step(action)\n",
    "      #print(returnValue)\n",
    "      # returnValue[0]: observation (object) \n",
    "      # returnValue[1]: reward that is the result of taking the action\n",
    "      # returnValue[3]: terminated (bool)     - is it a terminal state\n",
    "      # returnValue[4]: truncated (bool)      - it is not important in our case\n",
    "      # returnValue[5]: info (dictionary)     - in our case transi\n",
    "      steps+=1\n",
    "      if returnValue[2] and returnValue[3] == 1:\n",
    "        #print('You have got the Frisbee after {} steps'.format(steps))\n",
    "        steps_list.append(steps)\n",
    "        break\n",
    "      elif returnValue[2] and returnValue[3] == 0:\n",
    "        #print(\"You fell in a hole!\")\n",
    "        misses += 1\n",
    "        break\n",
    "    all_steps_list.append(steps)\n",
    "  print(steps_list)\n",
    "  print(misses)\n",
    "  print('----------------------------------------------')\n",
    "  print('You took an average of {:.0f} steps each episode'.format(np.mean(all_steps_list)))\n",
    "  print('You took an average of {:.0f} steps to get the frisbee'.format(np.mean(steps_list)))\n",
    "  print('And you fell in the hole {:.2f} % of the times'.format((misses/episodes) * 100))\n",
    "  print('----------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(obs):\n",
    "    #Your code here\n",
    "\n",
    "#Call the function above with this line. Try 100,1000,10000 episodes. How did you do?\n",
    "get_score(env,random_policy,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76310151",
   "metadata": {},
   "source": [
    "Discussion here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc1087a",
   "metadata": {},
   "source": [
    "## Part 3. Not So Random Policy (10 pts)\n",
    "\n",
    "Write another function called nonRandom_policy(obs). Come up with a rule that might choose one action more than another. You may use observation data if you would like. Assess it similarly to the code block above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d296284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonRandom_policy(obs):\n",
    "    #Your code here\n",
    "\n",
    "#Call the function above with this line. Try 100,1000,10000 episodes. How did you do?\n",
    "get_score(env,nonRandom_policy,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3b7ba",
   "metadata": {},
   "source": [
    "Discussion here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f833c7",
   "metadata": {},
   "source": [
    "# Part 4. Q-Learning Exploration (25 pts)\n",
    "\n",
    "Below is some code that will eventually train our elf on how to best navigate the Frozen Pond. Please work through each code block and \"reverse-engineer\" this solution. What is Q-Learning? Explain in your own words, what these code\n",
    "blocks are doing.\n",
    "\n",
    "Source: https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250b5fb-6ade-495d-b9e0-4c40d5a66d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\",render_mode=\"rgb_array\")\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2f5fd-84a3-488a-86bd-39d456b8642d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)\n",
    "\n",
    "total_episodes = 15000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.005             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1094a149-65fb-4bb2-ac8f-e666cd8f0bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state_init = env.reset()\n",
    "    state = state_init[0]\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7987ece4-dbea-43e0-a761-40860418666a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state_init = env.reset()\n",
    "    state = state_init[0]\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, trunc,info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            #env.render()\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55f2b7f-a6b6-4538-afeb-146b47bf1a0d",
   "metadata": {},
   "source": [
    "Your thoughts/discussion here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3502e916-2b77-4a86-ac5b-a13f9cddd880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
